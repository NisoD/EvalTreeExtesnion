# MMLU-Specific Weakness Insights: DOVE vs Binary Evaluation
## Actual Subject-Area Discoveries and Robustness Differentiations

---

## üéØ **Key Discovery: STEM Shows Critical Weakness Pattern**

### **STEM Category Analysis (19 subjects, 1,843 questions)**
- **DOVE Mean Performance**: 49.2% (High Weakness)
- **Binary Mean Performance**: 45.5% (Weak)
- **Robustness Difference**: 3.7%
- **Critical Finding**: STEM is the **only category classified as "High Weakness"** by DOVE

### **Non-STEM Categories Show Moderate Performance**
- **Humanities**: 56.7% (Moderate) - Perfect agreement between methods
- **Social Sciences**: 64.5% (Moderate) - 3.0% difference  
- **Professional**: 62.7% (Moderate) - 4.4% difference

---

## üî¨ **Top 10 Subject-Specific Discoveries**

### **1. Computer Security - MOST CRITICAL**
- **DOVE Score**: 29.8% (Critical Weakness)
- **Robustness Difference**: 14.4% (Highest divergence)
- **Category**: STEM
- **Insight**: Binary evaluation significantly underestimates this critical security weakness

### **2. Abstract Algebra - Mathematical Reasoning Gap**
- **DOVE Score**: 29.5% (Critical Weakness)  
- **Robustness Difference**: 9.9%
- **Category**: STEM
- **Insight**: Fundamental mathematical reasoning shows severe weakness

### **3. High School Physics - Basic STEM Foundation**
- **DOVE Score**: 39.2% (High Weakness)
- **Robustness Difference**: 12.4% (Second highest)
- **Category**: STEM
- **Insight**: Even basic physics concepts show significant weakness

### **4. Electrical Engineering - Technical Application**
- **DOVE Score**: 33.6% (High Weakness)
- **Robustness Difference**: 9.9%
- **Category**: STEM
- **Insight**: Applied engineering knowledge has critical gaps

### **5. High School Statistics - Data Analysis**
- **DOVE Score**: 39.0% (High Weakness)
- **Robustness Difference**: 11.2%
- **Category**: STEM
- **Insight**: Statistical reasoning shows concerning weakness

### **6. Philosophy - Logical Reasoning**
- **DOVE Score**: 45.3% (High Weakness)
- **Robustness Difference**: 9.2%
- **Category**: Humanities
- **Insight**: Abstract philosophical reasoning challenges revealed

### **7. Moral Scenarios - Ethical Reasoning**
- **DOVE Score**: 43.9% (High Weakness)
- **Robustness Difference**: 8.8%
- **Category**: Humanities  
- **Insight**: Complex ethical decision-making shows weakness

### **8-10. Professional Subjects with Robustness Issues**
- **Business Ethics**: 66.3% (7.9% difference)
- **Nutrition**: 66.3% (7.9% difference)
- **High School World History**: 67.3% (7.9% difference)

---

## üè• **Medical Sciences Deep Dive**

### **Medical Category Performance (10 subjects analyzed)**
- **Overall Assessment**: Generally moderate performance
- **Critical Weaknesses**: 0 subjects
- **High Weaknesses**: 1 subject
- **Key Finding**: Medical sciences are relatively robust compared to STEM

### **Specific Medical Subject Performance**:
1. **Virology** (Infectious Diseases): Moderate performance
2. **Medical Genetics**: Moderate performance  
3. **Clinical Knowledge**: Moderate performance
4. **Professional Medicine**: Moderate performance
5. **Anatomy**: Moderate performance
6. **College Medicine**: Moderate performance
7. **Human Aging**: Moderate performance
8. **Nutrition**: Moderate performance (but 7.9% robustness difference)

### **Medical Sciences Insight**:
**Contrary to expectation**, medical sciences do NOT show the critical weaknesses seen in core STEM subjects. This suggests:
- Medical knowledge application is more robust than fundamental STEM reasoning
- Clinical training may provide better performance consistency
- Applied medical knowledge differs from theoretical STEM understanding

---

## üìä **Category-Level Robustness Differentiation**

### **STEM Category - Most Vulnerable**
- **17 subjects with weaknesses** (Critical + High)
- **Average robustness difference**: 5.2%
- **Key weakness areas**:
  - Computer Security (Critical)
  - Abstract Mathematics (Critical)  
  - Physics fundamentals (High)
  - Engineering applications (High)
  - Statistical reasoning (High)

### **Professional Category - Moderate Robustness Issues**
- **4.4% average robustness difference** (highest among non-STEM)
- **Key insight**: Applied professional knowledge shows some evaluation inconsistencies
- **Notable subjects**: Business Ethics, Nutrition, Management

### **Social Sciences - Stable Performance**
- **3.0% average robustness difference**
- **Moderate performance** across most subjects
- **Consistent evaluation** between DOVE and binary methods

### **Humanities - Most Consistent**
- **0.0% average robustness difference** (perfect agreement)
- **Exception**: Philosophy and Moral Scenarios show individual subject differences
- **Overall stability** in evaluation methods

---

## üéØ **Targeted Question Generation Insights**

### **High-Priority Targets for Question Generation**

#### **1. STEM Subjects (Guaranteed Failure Areas)**
- **Computer Security**: 29.8% performance - generate cybersecurity scenario questions
- **Abstract Algebra**: 29.5% performance - create advanced mathematical reasoning problems
- **Electrical Engineering**: 33.6% performance - develop technical application questions
- **High School Physics**: 39.2% performance - focus on fundamental physics concepts

#### **2. Reasoning-Heavy Subjects**
- **Philosophy**: 45.3% performance - generate logical reasoning challenges
- **Moral Scenarios**: 43.9% performance - create complex ethical dilemmas
- **High School Statistics**: 39.0% performance - develop statistical analysis problems

### **Medium-Priority Targets**
- **Professional subjects** with robustness differences >7%
- **Applied knowledge areas** showing evaluation inconsistencies

---

## üîç **Method Comparison Revelations**

### **Where DOVE Reveals Hidden Weaknesses**
1. **Computer Security**: Binary misses 14.4% of weakness severity
2. **High School Physics**: Binary underestimates by 12.4%
3. **High School Statistics**: Binary misses 11.2% of difficulty
4. **Abstract Algebra**: Binary underestimates by 9.9%
5. **Electrical Engineering**: Binary misses 9.9% of complexity

### **Where Methods Agree (Stable Areas)**
- **Humanities overall**: Perfect agreement (0.0% difference)
- **Most medical sciences**: Consistent evaluation
- **Social sciences**: Minor differences (3.0% average)

### **Evaluation Reliability Patterns**
- **STEM subjects**: Higher variance, more evaluation disagreement
- **Applied subjects**: Moderate robustness differences  
- **Theoretical humanities**: Most consistent evaluation
- **Medical/clinical**: Surprisingly stable performance

---

## üìà **Research Contributions Demonstrated**

### **1. STEM Weakness Discovery**
- **Novel insight**: STEM subjects show systematically higher weakness rates
- **Specific targets**: Computer Security and Abstract Algebra as critical weakness areas
- **Practical impact**: Focus question generation on fundamental STEM reasoning

### **2. Medical Sciences Robustness**
- **Unexpected finding**: Medical subjects more robust than expected
- **Clinical vs theoretical**: Applied medical knowledge shows better consistency
- **Question generation insight**: Medical scenarios may be less effective for failure induction

### **3. Category-Specific Evaluation Patterns**
- **STEM**: High variance, significant robustness differences
- **Professional**: Moderate inconsistencies in applied knowledge
- **Humanities**: Most stable evaluation agreement
- **Social Sciences**: Consistent moderate performance

### **4. Subject-Specific Targeting**
- **Computer Security**: Highest priority for adversarial question generation
- **Mathematical reasoning**: Abstract Algebra shows fundamental gaps
- **Physics concepts**: Even basic physics reveals significant weaknesses
- **Ethical reasoning**: Philosophy and moral scenarios show complexity challenges

---

## üéØ **Actionable Insights for Research**

### **Immediate Question Generation Focus**
1. **Computer Security scenarios** (29.8% performance, 14.4% robustness gap)
2. **Abstract mathematical proofs** (29.5% performance)
3. **Physics problem-solving** (39.2% performance, 12.4% gap)
4. **Statistical analysis tasks** (39.0% performance, 11.2% gap)

### **Research Hypothesis Validation**
- **STEM weakness hypothesis**: ‚úÖ CONFIRMED - STEM shows systematic weaknesses
- **Medical complexity hypothesis**: ‚ùå REJECTED - Medical sciences are robust
- **Applied vs theoretical**: ‚úÖ CONFIRMED - Applied knowledge more consistent

### **Method Validation**
- **DOVE sensitivity**: Successfully identifies nuanced STEM weaknesses
- **Binary limitations**: Misses critical security and physics weaknesses  
- **Evaluation consistency**: Varies significantly by subject domain

---

## üìä **Statistical Summary by Category**

| Category | DOVE Mean | Binary Mean | Robustness Diff | Weakness Level | Key Insight |
|----------|-----------|-------------|-----------------|----------------|-------------|
| **STEM** | 49.2% | 45.5% | 3.7% | High | **Most vulnerable category** |
| **Professional** | 62.7% | 67.1% | 4.4% | Moderate | Applied knowledge inconsistencies |
| **Social Sciences** | 64.5% | 67.6% | 3.0% | Moderate | Stable performance |
| **Humanities** | 56.7% | 56.7% | 0.0% | Moderate | **Perfect method agreement** |

### **Critical Subject Discoveries**
- **Computer Security**: 29.8% (Critical) - 14.4% robustness gap
- **Abstract Algebra**: 29.5% (Critical) - 9.9% robustness gap  
- **Electrical Engineering**: 33.6% (High) - 9.9% robustness gap
- **High School Physics**: 39.2% (High) - 12.4% robustness gap

This analysis reveals that **STEM subjects, particularly Computer Security and Abstract Mathematics, represent the most critical weakness areas** where traditional binary evaluation significantly underestimates model limitations. These findings provide specific, actionable targets for generating questions that will reliably induce model failures.